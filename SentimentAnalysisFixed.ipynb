{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysisFixed_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tfAVgbIMYV5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-4OI523GI9h",
        "outputId": "741729ee-f865-4bec-ed86-79eae1b99a78"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW_-a8IiGxo2",
        "outputId": "c339429a-d00d-436c-960d-7656aaef01ef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiOlgPkXGy0m"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "import torch\n",
        "import transformers\n",
        "import seaborn as sns\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceTkxfirMht0"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DxdnZvZG3eV"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/dev/sentiment.csv\", names=['text', 'humor', 'category'])\n",
        "df['category'] = df['category'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3oXTSMdM84U",
        "outputId": "78688a43-864c-4366-df55-ff39ec7d2aa7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "RANDOM_SEED = np.random.randint(0, 1000)\n",
        "print('random seed: ', RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.25, random_state=RANDOM_SEED)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.7, random_state=RANDOM_SEED)\n",
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random seed:  596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1001, 3), (100, 3), (234, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c-DhdDXR7gM"
      },
      "source": [
        "# Model Creation & Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cidvcQDHD6P"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "# import transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO94ZTcXIg22"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lFzHkTkNzyk"
      },
      "source": [
        "## The Dataset for sentiment analysis\n",
        "\n",
        "*  All the tokens are padded to MAX_LEN value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfszsusXIxvi"
      },
      "source": [
        "class SentimentAnalysisDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, texts, labels, tokenizer, max_length, batch_size):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_length\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.texts[item])\n",
        "    label = self.labels[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'text' : text,\n",
        "        'input_ids' : encoding['input_ids'].flatten(),\n",
        "        'attention_mask' : encoding['attention_mask'].flatten(),\n",
        "        'labels' : torch.tensor(label, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2I5DUf5N9xm"
      },
      "source": [
        "## The DataLoader for the SentimentAnalysis dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT-ptAtXI3nD"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_length, batch_size):\n",
        "  ds = SentimentAnalysisDataset(\n",
        "      texts=df.text.to_numpy(),\n",
        "      labels=df.humor.to_numpy(),\n",
        "      tokenizer=tokenizer,\n",
        "      max_length=max_length,\n",
        "      batch_size=batch_size\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=2\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDfhTbII9cZ"
      },
      "source": [
        "MAX_LEN = 50\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvJymAmcODjV"
      },
      "source": [
        "## The classifier consists of a:\n",
        "\n",
        "*   Pre-Trained BERT Model\n",
        "*   Dropout Layer (*p=0.1*)\n",
        "*   Fully Connected Layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXq2sPphI_Ye"
      },
      "source": [
        "class SentimentClassifier(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = torch.nn.Dropout(p=0.1)\n",
        "    self.out = torch.nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    returned = self.bert(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "    pooled_output = returned.pooler_output\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryvoXO-TLn5N",
        "outputId": "c9aa601e-adad-40f1-cb47-1bebc547c211"
      },
      "source": [
        "model = SentimentClassifier(3)\n",
        "device = torch.device(type='cuda', index=0)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d5YBngYLsYy"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "EPOCHS = 6\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=2e-5,\n",
        "                  eps = 1e-6,\n",
        "                  correct_bias=True,\n",
        "                  weight_decay=0.01)\n",
        "\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=warmup_steps,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = CrossEntropyLoss().to(device) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7uhGryzLt4i"
      },
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for batch in data_loader:\n",
        "    b_input_ids = batch['input_ids'].to(device)\n",
        "    b_att_mask = batch['attention_mask'].to(device)\n",
        "    b_labels = batch['labels'].to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    logits = model(b_input_ids, b_att_mask)\n",
        "\n",
        "    _, preds = torch.max(logits, dim=1)\n",
        "    loss = loss_fn(logits, b_labels)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == b_labels)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwQ5Q2Q0Lvoh"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      b_input_ids = batch['input_ids'].to(device)\n",
        "      b_att_mask = batch['attention_mask'].to(device)\n",
        "      b_labels = batch['labels'].to(device)\n",
        "\n",
        "      logits = model(b_input_ids, b_att_mask)\n",
        "\n",
        "      _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "      loss = loss_fn(logits, b_labels)\n",
        "      \n",
        "      losses.append(loss.item())\n",
        "      correct_predictions += torch.sum(preds == b_labels)\n",
        "      \n",
        "  print('Final preds: ', correct_predictions.double())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gITMKqqBLxGR"
      },
      "source": [
        "def predict_labels(model, data_loader):\n",
        "  \n",
        "  predicted_labels = []\n",
        "  real_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "\n",
        "      b_input_ids = batch['input_ids'].to(device)\n",
        "      b_att_mask = batch['attention_mask'].to(device)\n",
        "      b_labels = batch['labels'].to(device)\n",
        "\n",
        "      logits = model(b_input_ids, b_att_mask)\n",
        "\n",
        "      _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "      probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "      predicted_labels.extend(preds)\n",
        "      real_labels.extend(b_labels)\n",
        "  \n",
        "  predicted_labels = torch.stack(predicted_labels).cpu()\n",
        "  real_labels = torch.stack(real_labels).cpu()\n",
        "  return real_labels, predicted_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6zQu4dVSW_9"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMngOWcbL9um",
        "outputId": "fcfb2565-2146-40d0-d40d-22af246cdeba"
      },
      "source": [
        "%%time\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print('-' * 20)\n",
        "  print('Epoch: ', epoch+1)\n",
        "  print('-' * 20)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(model,\n",
        "                                      train_data_loader,\n",
        "                                      loss_fn,\n",
        "                                      optimizer,\n",
        "                                      device,\n",
        "                                      scheduler,\n",
        "                                      len(df_train))\n",
        "  \n",
        "  print('Train loss: ', train_loss)\n",
        "  print('Train acc: ', train_acc.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "Epoch:  1\n",
            "--------------------\n",
            "Train loss:  0.8486177240099225\n",
            "Train acc:  0.5724275724275725\n",
            "--------------------\n",
            "Epoch:  2\n",
            "--------------------\n",
            "Train loss:  0.4819904636769068\n",
            "Train acc:  0.7832167832167832\n",
            "--------------------\n",
            "Epoch:  3\n",
            "--------------------\n",
            "Train loss:  0.3164437946582597\n",
            "Train acc:  0.8841158841158842\n",
            "--------------------\n",
            "Epoch:  4\n",
            "--------------------\n",
            "Train loss:  0.1801791617439853\n",
            "Train acc:  0.9370629370629371\n",
            "--------------------\n",
            "Epoch:  5\n",
            "--------------------\n",
            "Train loss:  0.0804999265967617\n",
            "Train acc:  0.981018981018981\n",
            "--------------------\n",
            "Epoch:  6\n",
            "--------------------\n",
            "Train loss:  0.06113733179749004\n",
            "Train acc:  0.985014985014985\n",
            "CPU times: user 1min 3s, sys: 2.35 s, total: 1min 6s\n",
            "Wall time: 1min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EyXDzAqSYoM"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4IOHtSDL_Y5",
        "outputId": "39149aa4-b447-4e48-e280-246c258c6062"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final preds:  tensor(174., device='cuda:0', dtype=torch.float64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7435897435897436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAb6xlOWMD8O",
        "outputId": "03f9715f-069b-4aa6-e03e-03115decf192"
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
        "\n",
        "true_labels, predicted_labels = predict_labels(model, test_data_loader)\n",
        "\n",
        "f1_w = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "f1_micro = f1_score(true_labels, predicted_labels, average='micro')\n",
        "f1_macro = f1_score(true_labels, predicted_labels, average='macro')\n",
        "prec_rec_f1 = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "\n",
        "print('F1-Score Weighted: ', f1_w)\n",
        "print('F1-Score Micro: ', f1_micro)\n",
        "print('F1-Score Macro: ', f1_macro)\n",
        "print(prec_rec_f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-Score Weighted:  0.7414613845355712\n",
            "F1-Score Micro:  0.7435897435897437\n",
            "F1-Score Macro:  0.7398651152449418\n",
            "(0.746448023767221, 0.7435897435897436, 0.7414613845355712, None)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}